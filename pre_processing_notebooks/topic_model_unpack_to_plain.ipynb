{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be64095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install top2vec\n",
    "# %pip install umap\n",
    "# %pip install scipy==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from top2vec import Top2Vec\n",
    "import json\n",
    "from pathlib import Path\n",
    "from umap.umap_ import UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.home() / \"SEED_DATA/impact_and_fiction\"\n",
    "model_whole = Top2Vec.load(BASE_DIR / \"t2v_model-speed_learn-book_lemmas-uncased-min_df_0.01-max_df_0.5-content.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_whole.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e165052",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model_whole.get_topic_sizes()\n",
    "df = pd.DataFrame(topic_sizes, index=topic_nums, columns=[\"topic_size\"])\n",
    "print(f\"Num of Documents = {df['topic_size'].sum()}\")\n",
    "total_topics_found = df.shape[0]\n",
    "print(f\"Num of Topics = {total_topics_found}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa49769",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model_whole.get_topics(total_topics_found)\n",
    "topic_dict = [{\"topic_words\": row_tw[:20], \"word_scores\": row_sc[:20]} for row_tw, row_sc in zip(topic_words, word_scores)]\n",
    "df_topic_words = pd.DataFrame(topic_dict, index=topic_nums, columns=[\"topic_words\", \"word_scores\"])\n",
    "df_sw = pd.concat([df, df_topic_words], axis=1) \n",
    "df_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_documents = []\n",
    "doc2topic = {} # defaultdict(set)\n",
    "for topic_id, topic_row in df_sw.iterrows():\n",
    "    document_scores, document_ids = model_whole.search_documents_by_topic(topic_num=topic_id, num_docs=topic_row[\"topic_size\"])\n",
    "    isbn_only = [x.split(\"_\")[1] for x in document_ids]\n",
    "    topic_documents.append({\"doc_ids\": isbn_only, \"doc_scores\": list(document_scores)})\n",
    "    for score, doc_id in zip(document_scores, isbn_only):\n",
    "        doc2topic[doc_id] = topic_id \n",
    "        # print(f\"Document: {doc_id}, Score: {score}\")\n",
    "        # print(\"-----------\")\n",
    "df_topic_docs = pd.DataFrame(topic_documents, index=df_sw.index)\n",
    "df_topic_docs = pd.concat([df_sw, df_topic_docs], axis=1)\n",
    "\n",
    "df_topic_docs.to_csv(\"data/topic_docs_table.csv\", index=False)\n",
    "\n",
    "df_topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_whole.topic_vectors.shape)\n",
    "print(model_whole.document_vectors.shape)\n",
    "print(model_whole.word_vectors.shape)\n",
    "print(model_whole.document_ids.shape)\n",
    "\n",
    "umap = UMAP(verbose=True)\n",
    "reduced_document_vectors = umap.fit_transform(model_whole.document_vectors)\n",
    "reduced_document_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_book_duplicates(top2vec_doc_ids, doc_vecs, valid_files_mapping):\n",
    "    \"\"\"This exists because the top2vec was created with some duplicated books (e.g. different publishers of the same book).\n",
    "    In order to map it to only one ISBN and index it properly without loosing which vectors belong to which book we deduplicate here \n",
    "    using a predefined JSON file containing the list of deduplicated files\n",
    "\n",
    "    Args:\n",
    "        top2vec_doc_ids (_type_): The IDs as saved in the Top2Vec model\n",
    "        doc_vecs (_type_): The 2-Dim Topic Vectors\n",
    "        valid_files_mapping (_type_): Mapping from the actual deduplicated files in novels_tokens folder to toher properties\n",
    "    \"\"\"\n",
    "    clean_ids, clean_vecs = [], []\n",
    "    for doc_id, doc_vec in zip(top2vec_doc_ids, doc_vecs):\n",
    "        # doc_id now looks like this: '/home/wfa010/out/txt/IP1574938630912/20191029143048_9789045020860'\n",
    "        key = f\"{doc_id.split('/')[-1]}-tokens.txt.gz\"\n",
    "        clean_doc_id = doc_id.split('/')[-1].split(\"_\")[1]\n",
    "        if key in valid_files_mapping:\n",
    "            clean_ids.append(clean_doc_id)\n",
    "            clean_vecs.append(doc_vec)\n",
    "    \n",
    "    return clean_ids, clean_vecs\n",
    "\n",
    "valid_files_mapping = json.load(open(BASE_DIR / \"filename2isbn.json\"))\n",
    "clean_doc_ids, clean_doc_vecs = remove_book_duplicates(model_whole.document_ids, list(reduced_document_vectors), valid_files_mapping)\n",
    "print(len(clean_doc_ids))\n",
    "\n",
    "df_doc_vectors = pd.DataFrame({\"doc_vector\": clean_doc_vecs}, index=clean_doc_ids)\n",
    "df_doc2topic = pd.DataFrame(doc2topic.items(), columns=[\"doc_id\", \"topic_id\"])\n",
    "df_doc2topic = df_doc2topic.set_index(\"doc_id\")\n",
    "book_db = pd.concat([df_doc_vectors, df_doc2topic], axis=1)\n",
    "book_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isbn_nur(isbn_mappings_file):\n",
    "    dtype = {\n",
    "        'isbn': str,\n",
    "        'nur': str\n",
    "    }\n",
    "\n",
    "    temp_df = pd.read_csv(isbn_mappings_file, index_col=False, sep='\\t', dtype=dtype)\n",
    "    isbn_nur = temp_df[['isbn', 'nur']].set_index('isbn') # .drop_duplicates() ?\n",
    "    isbn_nur['nur'] = isbn_nur.nur.apply(lambda x: int(x) if isinstance(x, str) else 0)\n",
    "    return pd.get_dummies(isbn_nur.nur)\n",
    "\n",
    "BASE_DIR = Path.home() / \"SEED_DATA/impact_and_fiction\"\n",
    "isbn_mappings_file = BASE_DIR / \"work-isbn-mapping.tsv\"\n",
    "assert Path.is_file(isbn_mappings_file)  \n",
    "isbn_nur = get_isbn_nur(isbn_mappings_file)\n",
    "print(isbn_nur.shape)\n",
    "isbn_nur.to_csv(\"data/isbn_nur_table.csv\")\n",
    "isbn_nur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb91f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is not matching all of the ISBNs form the files with the isbn_nur_table (more than 14K are missing). So we skip this step. The next table has the nurs already.\n",
    "\n",
    "# def get_nur_from_isbn(isbn_nur_df, isbn):\n",
    "#     valid_row = isbn_nur_df[isbn_nur_df.index == isbn]\n",
    "#     if valid_row.empty:\n",
    "#         return []\n",
    "#     else:\n",
    "#         return valid_row.columns[isbn_nur_df.loc[isbn_nur_df.index == isbn].all(axis=0)].tolist()\n",
    "\n",
    "# # print(get_nur_from_isbn(isbn_nur, \"9789028426214\"))\n",
    "\n",
    "# book_db[\"nur_id\"] = book_db.apply(lambda row: get_nur_from_isbn(isbn_nur, row.name), axis=1)\n",
    "# book_db.to_csv(\"book_nur_table.csv\")\n",
    "# book_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_metadata = BASE_DIR / \"book-metadata.csv\"\n",
    "metadata_df = pd.read_csv(book_metadata)\n",
    "metadata_df = metadata_df.set_index('isbn')\n",
    "\n",
    "common_indices = metadata_df.index.intersection(book_db.index)\n",
    "metadata_df = metadata_df.loc[~metadata_df.index.duplicated(keep='first')]\n",
    "\n",
    "meta_filtered = metadata_df.loc[common_indices].sort_index()\n",
    "meta_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nur_labels_EN = {\n",
    "    0: 'unknown',\n",
    "    285: 'Young adult fiction',\n",
    "    300: 'literary fiction general',\n",
    "    301: 'Dutch literary novel, novella',\n",
    "    302: 'translated literary novel, novella',\n",
    "    305: 'literary thriller',\n",
    "    311: 'literary fiction, pocket',\n",
    "    312: 'popular fiction, pocket',\n",
    "    313: 'suspense pocket',\n",
    "    315: 'translated pocket',\n",
    "    330: 'suspense general',\n",
    "    331: 'detective',\n",
    "    332: 'thriller',\n",
    "    333: 'science fiction',\n",
    "    334: 'fantasy',\n",
    "    335: 'scary- and ghost stories, horror',\n",
    "    336: 'adventure novel',\n",
    "    337: 'war and resistance novel',\n",
    "    338: 'spy novel',\n",
    "    340: 'popular fiction general',\n",
    "    342: 'historical novel (popular)',\n",
    "    343: 'romance'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nur_names(nur_ids):\n",
    "    nur_names = []\n",
    "\n",
    "    if isinstance(nur_ids, str):\n",
    "        nur_ids = eval(nur_ids)\n",
    "    elif isinstance(nur_ids, list):\n",
    "        pass\n",
    "    else:\n",
    "        nur_ids = []\n",
    "\n",
    "    for nid in nur_ids:\n",
    "        nur_names.append(nur_labels_EN.get(int(nid), 'unknown'))\n",
    "    return nur_names\n",
    "\n",
    "\n",
    "metadata_df = meta_filtered[[\"title\", \"author\", \"publisher\", \"nur\"]]\n",
    "metadata_df = book_db.join(metadata_df)\n",
    "metadata_df[\"nur_names\"] = metadata_df.apply(lambda x: get_nur_names(x['nur']), axis=1)\n",
    "\n",
    "metadata_df.to_csv(\"data/book_topic.tsv\", sep=\"\\t\")\n",
    "\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Don't need this cell anymore but it is a very nice trick to merge duplicated rows into a single row with all values in a list per row\n",
    "\n",
    "# df_books = pd.read_csv(isbn_mappings_file, sep=\"\\t\")\n",
    "# print(df_books[df_books[\"isbn\"] == \"9789044630039\"])\n",
    "\n",
    "# df_books[df_books[\"isbn\"] == \"0312347324\"]\n",
    "# df_books[df_books[\"isbn\"] == \"9789023449348\"]\n",
    "\n",
    "# df_books_mini = df_books[[\"isbn\", \"title\", \"author\", \"publisher\"]]\n",
    "\n",
    "# def merge_to_list(group):\n",
    "#     id_value = group['isbn'].iloc[0]\n",
    "#     merged_values = [id_value]\n",
    "#     merged_values.append(group['title'].tolist())\n",
    "#     merged_values.append(group['author'].tolist())\n",
    "#     merged_values.append(group['publisher'].tolist())\n",
    "    \n",
    "#     return pd.Series(merged_values, index=['isbn', 'title', 'author', 'publisher'])\n",
    "\n",
    "# # Apply the function to each group\n",
    "# transformed_df = df_books_mini.groupby('isbn').apply(merge_to_list).reset_index(drop=True)\n",
    "\n",
    "# transformed_df[transformed_df[\"isbn\"] == \"9789044630039\"]\n",
    "# # transformed_df[transformed_df[\"isbn\"] == \"9789023449348\"]\n",
    "\n",
    "\n",
    "# author_counts = df_books[\"author\"].value_counts()\n",
    "# author_counts = author_counts[author_counts > 10]\n",
    "# author_counts.to_csv(\"author_unique_counts.csv\")\n",
    "\n",
    "\n",
    "# publisher_counts = df_books[\"publisher\"].value_counts()\n",
    "# publisher_counts = publisher_counts[publisher_counts > 10]\n",
    "# publisher_counts.to_csv(\"publisher_unique_counts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f11c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def force_explode(column_value):\n",
    "    if isinstance(column_value, str):\n",
    "        vals = eval(column_value)\n",
    "    elif isinstance(column_value, list):\n",
    "        pass\n",
    "    else:\n",
    "        vals = []\n",
    "    return pd.Series(vals)\n",
    "\n",
    "nur_counts = metadata_df['nur'].apply(force_explode).stack().value_counts()\n",
    "nur_counts = nur_counts[nur_counts > 2]\n",
    "nur_counts.to_csv(\"data/unique_counts_nur.csv\")\n",
    "\n",
    "author_counts = metadata_df['nur'].apply(force_explode).stack().value_counts()\n",
    "author_counts = author_counts[author_counts > 2]\n",
    "author_counts.to_csv(\"data/unique_counts_author.csv\")\n",
    "\n",
    "publisher_counts = metadata_df['nur'].apply(force_explode).stack().value_counts()\n",
    "publisher_counts = publisher_counts[publisher_counts > 2]\n",
    "publisher_counts.to_csv(\"data/unique_counts_publisher.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
