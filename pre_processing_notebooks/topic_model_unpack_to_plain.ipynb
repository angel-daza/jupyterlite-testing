{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f36c561",
   "metadata": {},
   "source": [
    "## Load Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be64095",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Just a reminder of the minimal libraries that we need for pre-processing\n",
    "# %pip install pandas\n",
    "# %pip install top2vec\n",
    "# %pip install umap\n",
    "# %pip install scipy==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from top2vec import Top2Vec\n",
    "import json\n",
    "from pathlib import Path\n",
    "from umap.umap_ import UMAP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0d5423",
   "metadata": {},
   "source": [
    "## Load the Top2Vec Model and transfer to Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5980ab8",
   "metadata": {},
   "source": [
    "#### Get the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.home() / \"SEED_DATA/impact_and_fiction\"\n",
    "model_whole = Top2Vec.load(BASE_DIR / \"t2v_model-speed_learn-book_lemmas-uncased-min_df_0.01-max_df_0.5-content.model\")\n",
    "model_whole.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7604ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_whole.topic_vectors.shape)\n",
    "print(model_whole.document_vectors.shape)\n",
    "print(model_whole.word_vectors.shape)\n",
    "print(model_whole.document_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55bde5a",
   "metadata": {},
   "source": [
    "#### Get the Topic Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e165052",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model_whole.get_topic_sizes()\n",
    "df = pd.DataFrame(topic_sizes, index=topic_nums, columns=[\"topic_size\"])\n",
    "print(f\"Num of Documents = {df['topic_size'].sum()}\")\n",
    "total_topics_found = df.shape[0]\n",
    "print(f\"Num of Topics = {total_topics_found}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16890223",
   "metadata": {},
   "source": [
    "#### Get the most frequent topic words and their scores per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa49769",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model_whole.get_topics(total_topics_found)\n",
    "topic_dict = [{\"topic_words\": row_tw[:20], \"word_scores\": row_sc[:20]} for row_tw, row_sc in zip(topic_words, word_scores)]\n",
    "df_topic_words = pd.DataFrame(topic_dict, index=topic_nums, columns=[\"topic_words\", \"word_scores\"])\n",
    "df_score_words = pd.concat([df, df_topic_words], axis=1) \n",
    "df_score_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ad5ed",
   "metadata": {},
   "source": [
    "#### Get the Document IDs (ISBNs) that belong to each ropic and their belongness scores (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topic_documents = []\n",
    "doc2topic = {} # defaultdict(set)\n",
    "for topic_id, topic_row in df_score_words.iterrows():\n",
    "    document_scores, document_ids = model_whole.search_documents_by_topic(topic_num=topic_id, num_docs=topic_row[\"topic_size\"])\n",
    "    isbn_only = [x.split(\"_\")[1] for x in document_ids]\n",
    "    topic_documents.append({\"doc_ids\": isbn_only, \"doc_scores\": list(document_scores)})\n",
    "    for score, doc_id in zip(document_scores, isbn_only):\n",
    "        doc2topic[doc_id] = topic_id \n",
    "        # print(f\"Document: {doc_id}, Score: {score}\")\n",
    "        # print(\"-----------\")\n",
    "df_topic_docs = pd.DataFrame(topic_documents, index=df_score_words.index)\n",
    "df_topic_docs = pd.concat([df_score_words, df_topic_docs], axis=1)\n",
    "\n",
    "df_topic_docs.to_csv(\"data/topic_docs_table.csv\", index=True)\n",
    "\n",
    "df_topic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2af04",
   "metadata": {},
   "source": [
    "#### Compress the Document Vectors into 2-dimensions using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(verbose=True)\n",
    "reduced_document_vectors = umap.fit_transform(model_whole.document_vectors)\n",
    "reduced_document_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a91d31",
   "metadata": {},
   "source": [
    "#### Filter and Clean the Table \n",
    "So we only keep the deduplicated books that are in `novels_tokens`.\n",
    "We also create the division between `Non-fiction` and `Fiction` books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd64d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_book_duplicates(top2vec_doc_ids, doc_vecs, valid_files_mapping):\n",
    "    \"\"\"This exists because the top2vec was created with some duplicated books (e.g. different publishers of the same book).\n",
    "    In order to map it to only one ISBN and index it properly without loosing which vectors belong to which book we deduplicate here \n",
    "    using a predefined JSON file containing the list of deduplicated files\n",
    "\n",
    "    Args:\n",
    "        top2vec_doc_ids (_type_): The IDs as saved in the Top2Vec model\n",
    "        doc_vecs (_type_): The 2-Dim Topic Vectors\n",
    "        valid_files_mapping (_type_): Mapping from the actual deduplicated files in novels_tokens folder to toher properties\n",
    "    \"\"\"\n",
    "    clean_ids, clean_vecs_x, clean_vecs_y = [], [], []\n",
    "    for doc_id, doc_vec in zip(top2vec_doc_ids, doc_vecs):\n",
    "        # doc_id now looks like this: '/home/wfa010/out/txt/IP1574938630912/20191029143048_9789045020860'\n",
    "        key = f\"{doc_id.split('/')[-1]}-tokens.txt.gz\"\n",
    "        clean_doc_id = doc_id.split('/')[-1].split(\"_\")[1]\n",
    "        if key in valid_files_mapping:\n",
    "            clean_ids.append(clean_doc_id)\n",
    "            clean_vecs_x.append(doc_vec[0])\n",
    "            clean_vecs_y.append(doc_vec[1])\n",
    "    \n",
    "    return clean_ids, clean_vecs_x, clean_vecs_y\n",
    "\n",
    "valid_files_mapping = json.load(open(BASE_DIR / \"filename2isbn.json\"))\n",
    "clean_doc_ids, clean_doc_vecs_x, clean_doc_vecs_y = remove_book_duplicates(model_whole.document_ids, list(reduced_document_vectors), valid_files_mapping)\n",
    "print(len(clean_doc_ids))\n",
    "\n",
    "df_doc_vectors = pd.DataFrame({\"doc_x\": clean_doc_vecs_x, \"doc_y\": clean_doc_vecs_y}, index=clean_doc_ids)\n",
    "df_doc2topic = pd.DataFrame(doc2topic.items(), columns=[\"doc_id\", \"topic_id\"])\n",
    "df_doc2topic = df_doc2topic.set_index(\"doc_id\")\n",
    "book_db = pd.concat([df_doc_vectors, df_doc2topic], axis=1)\n",
    "book_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e449",
   "metadata": {},
   "source": [
    "### Add NUR Information (Genres of interest) columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_isbn_nur(isbn_mappings_file):\n",
    "#     dtype = {\n",
    "#         'isbn': str,\n",
    "#         'nur': str\n",
    "#     }\n",
    "\n",
    "#     temp_df = pd.read_csv(isbn_mappings_file, index_col=False, sep='\\t', dtype=dtype)\n",
    "#     isbn_nur = temp_df[['isbn', 'nur']].set_index('isbn') # .drop_duplicates() ?\n",
    "#     isbn_nur['nur'] = isbn_nur.nur.apply(lambda x: int(x) if isinstance(x, str) else 0)\n",
    "#     return pd.get_dummies(isbn_nur.nur)\n",
    "\n",
    "# BASE_DIR = Path.home() / \"SEED_DATA/impact_and_fiction\"\n",
    "# isbn_mappings_file = BASE_DIR / \"work-isbn-mapping.tsv\"\n",
    "# assert Path.is_file(isbn_mappings_file)  \n",
    "# isbn_nur = get_isbn_nur(isbn_mappings_file)\n",
    "# print(isbn_nur.shape)\n",
    "# isbn_nur.to_csv(\"data/isbn_nur_table.csv\")\n",
    "# isbn_nur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb91f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is not matching all of the ISBNs form the files with the isbn_nur_table (more than 14K are missing). So we skip this step. The next table has the nurs already.\n",
    "\n",
    "# def get_nur_from_isbn(isbn_nur_df, isbn):\n",
    "#     valid_row = isbn_nur_df[isbn_nur_df.index == isbn]\n",
    "#     if valid_row.empty:\n",
    "#         return []\n",
    "#     else:\n",
    "#         return valid_row.columns[isbn_nur_df.loc[isbn_nur_df.index == isbn].all(axis=0)].tolist()\n",
    "\n",
    "# # print(get_nur_from_isbn(isbn_nur, \"9789028426214\"))\n",
    "\n",
    "# book_db[\"nur_id\"] = book_db.apply(lambda row: get_nur_from_isbn(isbn_nur, row.name), axis=1)\n",
    "# book_db.to_csv(\"book_nur_table.csv\")\n",
    "# book_db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e54ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_metadata = BASE_DIR / \"book-metadata.csv\"\n",
    "metadata_df = pd.read_csv(book_metadata)\n",
    "metadata_df = metadata_df.set_index('isbn')\n",
    "\n",
    "common_indices = metadata_df.index.intersection(book_db.index)\n",
    "metadata_df = metadata_df.loc[~metadata_df.index.duplicated(keep='first')]\n",
    "\n",
    "meta_filtered = metadata_df.loc[common_indices].sort_index()\n",
    "meta_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nur_labels_EN = {\n",
    "    0: 'unknown',\n",
    "    285: 'Young adult fiction',\n",
    "    300: 'literary fiction general',\n",
    "    301: 'Dutch literary novel, novella',\n",
    "    302: 'translated literary novel, novella',\n",
    "    305: 'literary thriller',\n",
    "    311: 'literary fiction, pocket',\n",
    "    312: 'popular fiction, pocket',\n",
    "    313: 'suspense pocket',\n",
    "    315: 'translated pocket',\n",
    "    330: 'suspense general',\n",
    "    331: 'detective',\n",
    "    332: 'thriller',\n",
    "    333: 'science fiction',\n",
    "    334: 'fantasy',\n",
    "    335: 'scary- and ghost stories, horror',\n",
    "    336: 'adventure novel',\n",
    "    337: 'war and resistance novel',\n",
    "    338: 'spy novel',\n",
    "    340: 'popular fiction general',\n",
    "    342: 'historical novel (popular)',\n",
    "    343: 'romance'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3706d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nur_names(nur_ids):\n",
    "    nur_names = []\n",
    "\n",
    "    if isinstance(nur_ids, str):\n",
    "        nur_ids = eval(nur_ids)\n",
    "    elif isinstance(nur_ids, list):\n",
    "        pass\n",
    "    else:\n",
    "        nur_ids = []\n",
    "\n",
    "    for nid in nur_ids:\n",
    "        if int(nid) in nur_labels_EN:\n",
    "            nur_names.append(nur_labels_EN.get(int(nid)))\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"Not found: {nid}\")\n",
    "    return nur_names\n",
    "\n",
    "\n",
    "metadata_df = meta_filtered[[\"title\", \"author\", \"publisher\", \"nur\"]]\n",
    "metadata_df = book_db.join(metadata_df)\n",
    "metadata_df[\"nur_names\"] = metadata_df.apply(lambda x: get_nur_names(x['nur']), axis=1)\n",
    "\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Don't need this cell anymore but it is a very nice trick to merge duplicated rows into a single row with all values in a list per row\n",
    "\n",
    "# df_books = pd.read_csv(isbn_mappings_file, sep=\"\\t\")\n",
    "# print(df_books[df_books[\"isbn\"] == \"9789044630039\"])\n",
    "\n",
    "# df_books[df_books[\"isbn\"] == \"0312347324\"]\n",
    "# df_books[df_books[\"isbn\"] == \"9789023449348\"]\n",
    "\n",
    "# df_books_mini = df_books[[\"isbn\", \"title\", \"author\", \"publisher\"]]\n",
    "\n",
    "# def merge_to_list(group):\n",
    "#     id_value = group['isbn'].iloc[0]\n",
    "#     merged_values = [id_value]\n",
    "#     merged_values.append(group['title'].tolist())\n",
    "#     merged_values.append(group['author'].tolist())\n",
    "#     merged_values.append(group['publisher'].tolist())\n",
    "    \n",
    "#     return pd.Series(merged_values, index=['isbn', 'title', 'author', 'publisher'])\n",
    "\n",
    "# # Apply the function to each group\n",
    "# transformed_df = df_books_mini.groupby('isbn').apply(merge_to_list).reset_index(drop=True)\n",
    "\n",
    "# transformed_df[transformed_df[\"isbn\"] == \"9789044630039\"]\n",
    "# # transformed_df[transformed_df[\"isbn\"] == \"9789023449348\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370021a",
   "metadata": {},
   "source": [
    "### Add the ISBN -> Work-Id Mapping\n",
    "\n",
    "The known books are the 18,467 valid files with the full book content inside `novels_tokens` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fd1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_isbn_path = BASE_DIR / \"work_isbn_title_genre.tsv\"\n",
    "work_isbn_db = pd.read_csv(work_isbn_path, sep=\"\\t\")\n",
    "work_isbn_db = work_isbn_db[work_isbn_db['record_id_type'] == 'isbn']\n",
    "\n",
    "work_isbn_db = work_isbn_db[['work_id', 'record_id']].set_index('record_id')\n",
    "work_isbn_db.index = work_isbn_db.index.str.strip()\n",
    "work_isbn_db.index.name = 'isbn'\n",
    "work_isbn_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Filter to only have the relevant ISBNs for the dataset (18,467)\n",
    "common_indices = metadata_df.index.intersection(work_isbn_db.index)\n",
    "relevant_work_isbns = work_isbn_db.loc[common_indices]\n",
    "relevant_work_isbns.to_csv(\"data/isbn_to_work_ids.csv\")\n",
    "\n",
    "# Save as dictionary to re-use later by other notebooks\n",
    "isbn_dict = relevant_work_isbns.to_dict(orient='index')\n",
    "isbn2workId = {k: v['work_id'] for k,v in isbn_dict.items()}\n",
    "workId2isbn = {v: k for k,v in isbn2workId.items()}\n",
    "json.dump(isbn2workId, open(\"data/isbn2workId.json\", \"w\", encoding='utf-8'))\n",
    "json.dump(workId2isbn, open(\"data/workId2isbn.json\", \"w\", encoding='utf-8'))\n",
    "\n",
    "relevant_work_isbns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89661c",
   "metadata": {},
   "source": [
    "## Final Book Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_element_as_list(elem, get_only_first):\n",
    "    if isinstance(elem, float): # NaN is a float\n",
    "        return elem\n",
    "    if isinstance(elem, str):\n",
    "        new_elem = eval(elem)\n",
    "    elif isinstance(elem, list):\n",
    "        new_elem = elem\n",
    "\n",
    "    # PROBLEMATIC CASES. How To solve it?    \n",
    "    # if len(new_elem) > 1:\n",
    "    #     print(sorted(new_elem))\n",
    "\n",
    "    if get_only_first and len(new_elem) > 0:\n",
    "        return sorted(new_elem)[0]\n",
    "    elif get_only_first and len(new_elem) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return new_elem\n",
    "\n",
    "    \n",
    "book_db = metadata_df.join(relevant_work_isbns, how='inner') # There are actuall only 18,465 valid records, the other two have NaN values anyway so we get rid of them...\n",
    "\n",
    "book_db['title'] = book_db['title'].apply(process_element_as_list, args=(True,))\n",
    "book_db['author'] = book_db['author'].apply(process_element_as_list, args=(True,))\n",
    "book_db['publisher'] = book_db['publisher'].apply(process_element_as_list, args=(True,)) ## WARN!!! Here we are arbitrarily choosing only the First PUBLISHER\n",
    "book_db['nur'] = book_db['nur'].apply(process_element_as_list, args=(False,))             ## WARN!!! Here we are arbitrarily choosing only the First NUR\n",
    "book_db['genre'] = book_db['nur_names'].apply(process_element_as_list, args=(True,)) ## WARN!!! Here we are arbitrarily choosing only the First NUR_NAME\n",
    "book_db\n",
    "\n",
    "\n",
    "\n",
    "book_db.to_csv(\"data/book_topic.tsv\", sep=\"\\t\")\n",
    "\n",
    "book_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8210f",
   "metadata": {},
   "source": [
    "### Get the Unique Values to use them later as drop-down filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f11c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We force the explode because sometimes the fields are stringified lists and they need to be python lists in order to `explode` \n",
    "def force_explode(column_value):\n",
    "    if isinstance(column_value, str):\n",
    "        vals = eval(column_value)\n",
    "    elif isinstance(column_value, list):\n",
    "        pass\n",
    "    else:\n",
    "        vals = []\n",
    "    return pd.Series(vals)\n",
    "\n",
    "nur_counts = metadata_df['nur'].apply(force_explode).stack().value_counts()\n",
    "nur_counts = nur_counts[nur_counts > 2]\n",
    "nur_counts.to_csv(\"data/unique_counts_nur.csv\")\n",
    "\n",
    "author_counts = metadata_df['nur'].apply(force_explode).stack().value_counts()\n",
    "author_counts = author_counts[author_counts > 2]\n",
    "author_counts.to_csv(\"data/unique_counts_author.csv\")\n",
    "\n",
    "publisher_counts = metadata_df['nur'].apply(force_explode).stack().value_counts()\n",
    "publisher_counts = publisher_counts[publisher_counts > 2]\n",
    "publisher_counts.to_csv(\"data/unique_counts_publisher.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
